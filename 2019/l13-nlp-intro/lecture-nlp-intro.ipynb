{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shestakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Introduction to Natural Language Processing<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrey.shestakov/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print(u'Так надо')\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Huge amount of information is represented in a form of natual language:\n",
    "    * Blog-posts\n",
    "    * Comments\n",
    "    * Review\n",
    "    * Search queries\n",
    "    * Code\n",
    "    * ...\n",
    "\n",
    "* Need a way to process this information:\n",
    "    * transfer \"characters\" to vectors\n",
    "    * recognize key entities in text\n",
    "    * understand relationships between them\n",
    "    * ...\n",
    "\n",
    "* **Computational linguistics** — is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "* Machine Translation\n",
    "* Information retrieval\n",
    "* Text summarization\n",
    "* Text Classification\n",
    "    * Spam filtering\n",
    "    * Semantic analysis\n",
    "    * Multilabel classification\n",
    "* Text Categorization\n",
    "* Named-entity recognition\n",
    "* QA-systems and assistants\n",
    "* Text Generation\n",
    "* Spell-checking and next-word suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine Translation\n",
    "\n",
    "<center><img src='http://lurkmore.so/images/0/04/Epic_fail.jpg' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "<center><img src='https://hsto.org/getpro/habr/post_images/b73/6f7/a6c/b736f7a6c182da3285c81fcb8ce32d85.jpg' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## QA-systems and assistants\n",
    "\n",
    "<center><img src='https://cdn5.img.ria.ru/images/150662/35/1506623555.jpg'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text suggestion\n",
    "\n",
    "<center><img src='./img/search.png', width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text Generation\n",
    "\n",
    "* https://talktotransformer.com/\n",
    "* https://pdos.csail.mit.edu/archive/scigen/\n",
    "* https://sgeneri.ru/text-generator\n",
    "* https://yandex.ru/referats/\n",
    "* https://yandex.ru/autopoet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NLP is HARD!\n",
    "* Various linguistic uncertainties\n",
    "    * morphologic: \"мой\", \"три\", \"стекло\"\n",
    "    * phonetic: \"Надо ждать\" - \"Надо ж дать\"\n",
    "    * lexical: \"кран\", \"ключ\"\n",
    "    * syntactical: \"мужу изменять нельзя\"\n",
    "* Language is in dynamic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Levels of abstraction\n",
    "* Characters\n",
    "* Words\n",
    "* Sentences\n",
    "* Paragraphs\n",
    "* Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "* **Tokenization** - task of document decomposition on to **tokens** (elemental units)\n",
    "* Usually, word-based granularity is considered\n",
    "\n",
    "\n",
    "* \"Кролики - это не только ценный мех, но и 3-4 килограмма диетического, легкоусвояемого мяса\"\n",
    "* \"Через 1.5 часа поеду в Гусь-Хрустальный.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tokenization of words\n",
    "\n",
    "* Special **regular expressions** can be utilized to word this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Через\n",
      "1.5\n",
      "часа\n",
      "поеду\n",
      "в\n",
      "Гусь-Хрустальный.\n"
     ]
    }
   ],
   "source": [
    "line = u\"Через 1.5 часа поеду в Гусь-Хрустальный.\"\n",
    "for w in line.split(' '):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Через\n",
      "1\n",
      "5\n",
      "часа\n",
      "поеду\n",
      "в\n",
      "Гусь\n",
      "Хрустальный\n"
     ]
    }
   ],
   "source": [
    "line = u\"Через 1.5 часа поеду в Гусь-Хрустальный.\"\n",
    "tokenizer = RegexpTokenizer('\\w+| \\$ [\\d \\.]+ | S\\+')\n",
    "for w in tokenizer.tokenize(line):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Через\n",
      "1\n",
      ".\n",
      "5\n",
      "часа\n",
      "поеду\n",
      "в\n",
      "Гусь\n",
      "-\n",
      "Хрустальный\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "line = u\"Через 1.5 часа поеду в Гусь-Хрустальный.\"\n",
    "for w in wordpunct_tokenize(line):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "line = u'テキストで表示ロシア語'\n",
    "line = u'Nach der Wahrscheinlichkeitstheorie steckt alles in Schwierigkeiten!'\n",
    "\n",
    "# =("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tokenization of sentences\n",
    "* Also not clear\n",
    "* May use regex\n",
    "* Or some model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88 in New York.',\n",
       " 'Please buy me two of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = 'Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.'\n",
    "sent_tokenize(text, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Через 1.5 часа поеду в Гусь-Хрустальный.\n",
      "Куплю там квасу!\n"
     ]
    }
   ],
   "source": [
    "text = u\"Через 1.5 часа поеду в Гусь-Хрустальный. Куплю там квасу!\"\n",
    "for sent in sent_tokenize(text, language='english'):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## n-grams\n",
    "\n",
    "* Sometimes it is usefull to consider sequences of tokens\n",
    "* To catch context\n",
    "* To be robust to spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'prefer',\n",
       " 'cheese',\n",
       " 'sause',\n",
       " 'I prefer',\n",
       " 'prefer cheese',\n",
       " 'cheese sause',\n",
       " 'I prefer cheese',\n",
       " 'prefer cheese sause']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def word_grams(words, min_=1, max_=4):\n",
    "    s = []\n",
    "    for n in range(min_, max_):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(u\" \".join(str(i) for i in ngram))\n",
    "    return s\n",
    "\n",
    "word_grams(u\"I prefer cheese sause\".split(u\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "che\n",
      "hee\n",
      "ees\n",
      "ese\n",
      "se \n",
      "e s\n",
      " sa\n",
      "sau\n",
      "aus\n",
      "use\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "line = \"cheese sause\"\n",
    "\n",
    "for i in range(len(line) - n + 1):\n",
    "    print(line[i:i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two types of text normalization\n",
    "\n",
    "* **Lemmatization** - process of transformation of a word to its base form (normal form) based on dictionaries morphological analysis\n",
    "    * mystem, pymorphy\n",
    "* **Stemming** - rule-based process of reduction of a words to its stem based\n",
    "    * sses → ss (caresses → caress)\n",
    "    * ies → i (ponies → poni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']\n",
    "\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в петербург проходить митинг против передача исаакиевский собор рпц\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "text = 'в петербурге прошел митинг против передачи исаакиевского собора рпц'\n",
    "\n",
    "print(''.join(m.lemmatize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====\n",
      "norm_form: пожарница\n",
      "tag: NOUN,anim,femn sing,nomn\n",
      "score: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2 \n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "res = morph.parse(u'пожарница')\n",
    "for item in res:\n",
    "    print('====')\n",
    "    print(u'norm_form: {}'.format(item.normal_form))\n",
    "    print(u'tag: {}'.format(item.tag))\n",
    "    print(u'score: {}'.format(item.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other preprocessing techniques\n",
    "* high-frequency word (stop-words) removal\n",
    "    * Unions, prepositions, numerals, pronouns\n",
    "* low-frequency word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('russian')\n",
    "print(stopwords[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other preprocessing techniques\n",
    "* Switching to lowercase\n",
    "    * may loose meaning (US > us) or emotion (GREAT > great)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Document vectorization\n",
    "# Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Bag of words\n",
    "\n",
    "* Most common way to represent documents\n",
    "* Each **document** $d_i$ from **corpus** $D$ is represented with vector\n",
    "$$ d_i = (w_1^i, w_2^i, \\dots, w_N^i) $$\n",
    "* $w_j^i$ - weight of word $j$ in document $i$\n",
    "* word order is ignored\n",
    "\n",
    "<center><img src='img/bow.png'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Bag of words\n",
    "\n",
    "$w_j^i$ can be calculated as\n",
    "* Binary signal (does document contain the word)\n",
    "* Frequency of word $j$ in document $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tf - Idf\n",
    "\n",
    "* tf-idf (term frequency - inversed document frequency)\n",
    "    * tf(term, doc)\n",
    "        * Binary signal\n",
    "        * Frequency of word $j$ in document $i$\n",
    "        * Smoothed frequency $\\log(f_j^i + 1)$\n",
    "    * idf(term, corpus) = inversed document frequency if a word $j$ in corpus $D$: $\\log\\left(\\frac{|D|}{n_j}\\right)$\n",
    "        * $|D|$ - size of corpus\n",
    "        * $n_t$ - number of documents which contain word $j$\n",
    "        * idf measures how specifit a word is\n",
    "        \n",
    "\n",
    "$$tfidf = tf(term,doc) * idf(term, corpus)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/tfidf.jpg', width=1200><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bag of words\n",
    "\n",
    "* Additionaly, l1 or l2 normalization is applied to document vectors\n",
    "    * Normalization is usually good for models\n",
    "    * Maintain \"meaning\" of a document (for longer documents words' frequency are usually greater)\n",
    "\n",
    "\n",
    "* Ok, what we can do now with BoW?\n",
    "    * Put it as features in models (linear models, naive bayes)\n",
    "    * Use it to find similar documents (with cosine or jaccard distance)\n",
    "    * Compress it to \"topics\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Semantic Indexing - LSI\n",
    "### aka\n",
    "## Latent Semantic Analysis - LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LSA and feature reduction technique\n",
    "\n",
    "Idea (and implementation) of LSI is very similar to PCA\n",
    "* We want to compress sparse BOW-representation in to dense representation\n",
    "* We want this representation to save as much initial information and be meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "Every matrix $A$ of size $n \\times m$ and rank $r$ can be decomposed as:\n",
    "$$ A = U \\Sigma V^\\top ,$$\n",
    "where \n",
    "* $U$ - unitary matrix, consists of eigenvectors of $AA^\\top$\n",
    "* $V$ - unitary matrix, consists of eigenvectors of $A^\\top A$\n",
    "* $S$ - diagonal matrix with singular values $s_i = \\sqrt{\\lambda_i}$\n",
    "\n",
    "<center><img src='img/svd.jpg' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SVD via PCA\n",
    "Matrices $U$ and $V$ can be used for PCA\n",
    "$$ AV = U\\Sigma $$\n",
    "\n",
    "<center><img src='img/pca_svd.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LSA\n",
    "\n",
    "* Let's make SVD of BOW matrix\n",
    "\n",
    "<center><img src='img/lsa.jpg'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pros**\n",
    "* Easy\n",
    "* Can find similar words with $V$ and similar documents with $U$\n",
    "* LSA in search task is called Latent Semantic Indexing\n",
    "    * Query $q$ is also some sequence of words\n",
    "    * How to we find most similar documents in corpus with LSI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pros**\n",
    "* Simple\n",
    "* Can find similar words with $V$ and similar documents with $U$\n",
    "* LSA in search task is called Latent Semantic Indexing\n",
    "    * Query $q$ is also some sequence of words\n",
    "    * Pass q through the same preprocessing as main text\n",
    "    * Make BOW representation of it\n",
    "    * Make LSI representation with\n",
    "        $$ \\hat{q} = qV\\Sigma^{-1} $$\n",
    "\n",
    "**Cons**\n",
    "* Problems with interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "<center><img src='http://web.eecs.utk.edu/~mberry/sc95/gif/berry_table402.gif' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LSI space\n",
    "\n",
    "<center><img src='http://web.eecs.utk.edu/~mberry/sc95/gif/berry_figure433.gif' width=700></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Query LSI repsesentation\n",
    "\n",
    "<center><img src='http://web.eecs.utk.edu/~mberry/sc95/gif/berry_figure467.gif' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Query in space!!\n",
    "\n",
    "<center><img src='http://web.eecs.utk.edu/~mberry/sc95/gif/berry_figure485.gif' width=700></center>\n",
    "\n",
    "example from http://web.eecs.utk.edu/~mberry/sc95/sc95.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* LSA is just some strange dimention-reduction algorithm which is applied to data\n",
    "* If we consider topic modelling as a phenomenon - it is much deeper than simple compression\n",
    "\n",
    "<center><img src='img/LDA-concept.png'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic topic models\n",
    "\n",
    "<center><img src='img/topic-meme.jpg' width=900></cetner>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# But.. not today..\n",
    "\n",
    "## FYI\n",
    "* [David Blei - topic modelling](https://www.youtube.com/watch?v=DDq3OVp9dNA)\n",
    "* [Tutorial on probabilistic TM](http://machinelearning.ru/wiki/images/1/1f/Voron14aist.pdf)\n",
    "* [Gensim - LDA](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "* [BigARTM](https://bigartm.readthedocs.io/en/stable/)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
