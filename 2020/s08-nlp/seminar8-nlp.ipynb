{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h2><center>Seminar</center></h2>\n",
    "<h2><center>Introduction to Natural Language Processing</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in Russian \n",
    "(from https://www.kaggle.com/c/sentiment-analysis-in-russian/data)\n",
    "\n",
    "The goal is to estimate sentiment of news in russian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('Data/train.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  1957 \n",
      "\n",
      "Text: \n",
      " Медики рассказали о состоянии пострадавшего мужчины, на которого было совершено нападение возле отделения банка по Тимирязева. Как прокомментировали Tengrinews.kz в пресс-службе Управления здравоохранения Алматы, с места происшествия в службу скорой помощи обратились двое человек. \n",
      "\n",
      "«Одному из них на месте была оказана медицинская помощь. От госпитализации он отказался. Второй пациент был доставлен в больницу скорой неотложной помощи (БСНП) с сотрясением головного мозга, ушибленной раной головы. Состояние на данный момент оценивается ближе к удовлетворительному. Пока он проходит обследование в больнице», — сообщили в Управлении здравоохранения Алматы.  \n",
      "\n",
      "Напомним, в Алматы на пересечении улиц Тимирязева и Маркова возле БЦ «Алатау Гранд» произошла стрельба, ориентировочно в обеденное время. В здании расположены отделения банков «ВТБ» и «Сбербанк». \n",
      "\n",
      "В настоящее время полицейские разыскивают подозреваемых в стрельбе. По факту нападения в местном управлении внутренних дел начато досудебное расследование по статье 192 УК РК «Разбой». Создана специальная следственно-оперативная группа из числа опытных сотрудников подразделений криминальной полиции. В настоящий момент проводится комплекс оперативных и следственных мероприятий, направленных на установление личностей нападавших и их задержание. \n",
      "\n",
      "Ранее в «ДО АО Банк ВТБ (Казахстан)» прокомментировали нападение на мужчину. По данным пресс-службы банка, все отделения банка работают в штатном режиме с применением усиленных мер безопасности.   сотрудники полиции работают внутри отделения банка «ВТБ». Место происшествия не оцеплено. \n",
      "\n",
      " \n",
      "\n",
      "Sentiment:  negative \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show example\n",
    "num = 1 # 100 - pos\n",
    "\n",
    "print(\"ID: \",          data[num][\"id\"], \"\\n\")\n",
    "print(\"Text: \\n\",      data[num][\"text\"])\n",
    "print(\"Sentiment: \",   data[num][\"sentiment\"], \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and data cleaning\n",
    "\n",
    "Let's split each text into words (**tokenizations**) and remove all **stop words** and punctuation characters. **Stop words** are words that commonly used in texts and can be ignored losing the texts meaning.\n",
    "\n",
    "<center><img src=\"img/tokenization.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # for work with strings\n",
    "import nltk   # Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mihailh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get russian stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "# example of stop words\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punctuation characters\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define word tokenizer\n",
    "word_tokenizer = nltk.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    texts = []\n",
    "    targets = []\n",
    "\n",
    "    for item in data:\n",
    "        \n",
    "        # collect nlabels of news\n",
    "        if item['sentiment'] == 'negative':\n",
    "            targets.append(0)\n",
    "        else:\n",
    "            targets.append(1)\n",
    "        \n",
    "        text_lower = item['text'].lower() # convert words in a text to lower case\n",
    "        tokens     = word_tokenizer.tokenize(text_lower) # splits the text into tokens (words)\n",
    "        \n",
    "        # remove punct and stop words from tokens\n",
    "        tokens = [word for word in tokens if (word not in string.punctuation and word not in stop_words)]\n",
    "        \n",
    "        texts.append(tokens) # collect the text tokens\n",
    "    \n",
    "    return texts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokenization and data cleaning\n",
    "texts, y = process_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n",
      "Tokens:  ['медики', 'рассказали', 'состоянии', 'пострадавшего', 'мужчины']\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "i = 1\n",
    "print(\"Label: \", y[i])\n",
    "print(\"Tokens: \", texts[i][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words normalization\n",
    "\n",
    "Here we will consider 2 ways of words normalizing: **stemming** and **lemmatization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/stem2.svg\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "# define stemmer\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: медики, After: медик\n",
      "Before: рассказали, After: рассказа\n",
      "Before: состоянии, After: состоян\n",
      "Before: пострадавшего, After: пострада\n",
      "Before: мужчины, After: мужчин\n",
      "Before: которого, After: котор\n",
      "Before: совершено, After: соверш\n",
      "Before: нападение, After: нападен\n",
      "Before: возле, After: возл\n",
      "Before: отделения, After: отделен\n"
     ]
    }
   ],
   "source": [
    "# example of its work\n",
    "i = 1\n",
    "for aword in texts[i][:10]:\n",
    "    aword_stem = stemmer.stem(aword)\n",
    "    print(\"Before: %s, After: %s\" % (aword, aword_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization convert a word to its normal form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/lemm.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 # Морфологический анализатор\n",
    "\n",
    "# define lemmatizer :)\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: медики, After: медик\n",
      "Before: рассказали, After: рассказать\n",
      "Before: состоянии, After: состояние\n",
      "Before: пострадавшего, After: пострадавший\n",
      "Before: мужчины, After: мужчина\n",
      "Before: которого, After: который\n",
      "Before: совершено, After: совершить\n",
      "Before: нападение, After: нападение\n",
      "Before: возле, After: возле\n",
      "Before: отделения, After: отделение\n"
     ]
    }
   ],
   "source": [
    "# example of its work\n",
    "i = 1\n",
    "for aword in texts[i][:10]:\n",
    "    aword_norm = morph.parse(aword)[0].normal_form\n",
    "    print(\"Before: %s, After: %s\" % (aword, aword_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oscar goes to stemming!\n",
    "\n",
    "Stemming oscar speach:  Thanks to the academy for this prestigious award! I would like to thank all nlp developers that are lazy to use lematization and do not want to wait for too long. Thank you, thank you very much!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5939d851e44a8da5312ef034f2e65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8263), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply stemming to all texts\n",
    "for i in tqdm_notebook(range(len(texts))):           # tqdm_notebook creates the process bar below :)\n",
    "    text_stemmed = list(map(stemmer.stem, texts[i])) # apply stemming to each word in a text\n",
    "    texts[i] = ' '.join(text_stemmed)                # unite all stemmed words into a new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n",
      "Text: \n",
      " медик рассказа состоян пострада мужчин котор соверш нападен возл отделен банк тимирязев прокомментирова tengrinews kz пресс служб управлен здравоохранен алмат мест происшеств служб скор помощ обрат дво человек « одн мест оказа медицинск помощ госпитализац отказа втор пациент доставл больниц скор неотложн помощ бснп сотрясен головн мозг ушиблен ран голов состоян дан момент оценива ближ удовлетворительн пок проход обследован больниц », — сообщ управлен здравоохранен алмат напомн алмат пересечен улиц тимирязев марков возл бц « алата гранд » произошл стрельб ориентировочн обеден врем здан располож отделен банк « втб » « сбербанк ». настоя врем полицейск разыскива подозрева стрельб факт нападен местн управлен внутрен дел начат досудебн расследован стат 192 ук рк « разб ». созда специальн следствен оперативн групп числ опытн сотрудник подразделен криминальн полиц настоя момент провод комплекс оперативн следствен мероприят направлен установлен личност напада задержан ран « а банк втб казахста )» прокомментирова нападен мужчин дан пресс служб банк отделен банк работа штатн режим применен усилен мер безопасн сотрудник полиц работа внутр отделен банк « втб ». мест происшеств оцепл\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "i = 1\n",
    "print(\"Label: \",   y[i])\n",
    "print(\"Text: \\n\",  texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, test_texts, train_y, test_y = train_test_split(texts, y, test_size=0.33, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF measures importance of word in a corpus of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/tfidf.jpg\" width=\"800\"></center>\n",
    "Image from: http://filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016',\n",
       " 'банк',\n",
       " 'год',\n",
       " 'государств',\n",
       " 'государствен',\n",
       " 'казахста',\n",
       " 'казахстанск',\n",
       " 'компан',\n",
       " 'котор',\n",
       " 'млрд',\n",
       " 'национальн',\n",
       " 'нов',\n",
       " 'област',\n",
       " 'перв',\n",
       " 'президент',\n",
       " 'проект',\n",
       " 'работ',\n",
       " 'развит',\n",
       " 'республик',\n",
       " 'рк',\n",
       " 'сво',\n",
       " 'стран',\n",
       " 'такж',\n",
       " 'тенг',\n",
       " 'эт']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit TF-IDF on train texts\n",
    "vectorizer = TfidfVectorizer(max_features = 25) # select the top 25 words\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "# The top 25 words\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF to train and test texts\n",
    "train_X = vectorizer.fit_transform(train_texts)\n",
    "test_X  = vectorizer.fit_transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.50615647, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.28990537, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.32315937, 0.38382162, 0.63875621]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "train_X.todense()[:2] # show the first 2 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'стран': 21,\n",
       " 'казахстанск': 6,\n",
       " 'республик': 18,\n",
       " 'казахста': 5,\n",
       " 'такж': 22,\n",
       " 'развит': 17,\n",
       " 'котор': 8,\n",
       " '2016': 0,\n",
       " 'год': 3,\n",
       " 'национальн': 11,\n",
       " 'работ': 16,\n",
       " 'рк': 19,\n",
       " 'сво': 20,\n",
       " 'государствен': 4,\n",
       " 'проект': 15,\n",
       " 'област': 13,\n",
       " 'компан': 7,\n",
       " 'тенг': 23,\n",
       " 'млрд': 10,\n",
       " 'эт': 24,\n",
       " 'банк': 2,\n",
       " 'перв': 14,\n",
       " 'лет': 9,\n",
       " '2017': 1,\n",
       " 'нов': 12}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word - column in X accordance\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mihailh/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY = 0.8221488815548221\n",
      "ROC-AUC =  0.6436539410322453\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(test_X)\n",
    "proba  = model.predict_proba(test_X)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "print(\"ACCURACY = {}\".format(accuracy_score(test_y, predict)))\n",
    "print(\"ROC-AUC =  {}\".format(roc_auc_score(test_y, proba[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** 25 words are too small to estimate news sentiment properly. We need more words. But how will we deal with high dimensionalities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "LSA is just similar to PCA. It reduces dimension of the input matrix X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/lsa.jpg\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=40000,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit TF-IDF on train texts\n",
    "vectorizer = TfidfVectorizer(max_features = 40000)\n",
    "vectorizer.fit(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF to train and test texts\n",
    "train_X = vectorizer.transform(train_texts)\n",
    "test_X  = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5536, 40000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 40000 words. But it is too large for a classification model. Let's use LSA to reduce dimension. In sklearn LSA is TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=1000, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# fit SVD decomposition\n",
    "svd = TruncatedSVD(n_components = 1000)\n",
    "svd.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SVD to train and test samples\n",
    "train_svd_X = svd.transform(train_X)\n",
    "test_svd_X  = svd.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5536, 1000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svd_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(train_svd_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY = 0.8709204253758709\n",
      "ROC-AUC =  0.9207216299517325\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(test_svd_X)\n",
    "proba   = model.predict_proba(test_svd_X)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "print(\"ACCURACY = {}\".format(accuracy_score(test_y, predict)))\n",
    "print(\"ROC-AUC =  {}\".format(roc_auc_score(test_y, proba[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
