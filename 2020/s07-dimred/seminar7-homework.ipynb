{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h2><center>Homework: Feature Selection and Dimention Reduction. PCA </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Recognition by Voice\n",
    "\n",
    "This database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz (human vocal range).\n",
    "\n",
    "The following acoustic properties of each voice are measured and included within the CSV:\n",
    "\n",
    "* meanfreq: mean frequency (in kHz)\n",
    "* sd: standard deviation of frequency\n",
    "* median: median frequency (in kHz)\n",
    "* Q25: first quantile (in kHz)\n",
    "* Q75: third quantile (in kHz)\n",
    "* IQR: interquantile range (in kHz)\n",
    "* skew: skewness (see note in specprop description)\n",
    "* kurt: kurtosis (see note in specprop description)\n",
    "* sp.ent: spectral entropy\n",
    "* sfm: spectral flatness\n",
    "* mode: mode frequency\n",
    "* centroid: frequency centroid (see specprop)\n",
    "* peakf: peak frequency (frequency with highest energy)\n",
    "* meanfun: average of fundamental frequency measured across acoustic signal\n",
    "* minfun: minimum fundamental frequency measured across acoustic signal\n",
    "* maxfun: maximum fundamental frequency measured across acoustic signal\n",
    "* meandom: average of dominant frequency measured across acoustic signal\n",
    "* mindom: minimum of dominant frequency measured across acoustic signal\n",
    "* maxdom: maximum of dominant frequency measured across acoustic signal\n",
    "* dfrange: range of dominant frequency measured across acoustic signal\n",
    "* modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n",
    "* label: male or female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data sample\n",
    "data = pd.read_csv(\"data/voice.csv\")\n",
    "print(\"DataFrame shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = data.columns.drop(['label'])\n",
    "print(\"Feature names: \", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = data[feature_names].values\n",
    "y = 1. * (data['label'].values == 'male')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split + Standardization\n",
    "\n",
    "We perform standartization to provide you possibility to use any classifier you know without any difficulties induced by feature scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data into train and test samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Selection\n",
    "\n",
    "Generally, tasks below can be done with any classifier, but we ask you to provide answers with Decision Tree classifier with the following hyperparameters (don't touch this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (1 point)\n",
    "\n",
    "Find just **one** feature, that provides the best classification accuracy. What is the name of this feature? Save name in variable first_best_feature and go through assert\n",
    "\n",
    "Hint: in **for** loop use one feature in the sample to train the classifier. Then, calculate the classification accuracy on the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(first_best_feature == 'meanfun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (2 points)\n",
    "\n",
    "Now you know the first best feature of the sample. In this task you need to find the second best feature. What are the names of these features?\n",
    "\n",
    "Save answer in variables first_best_feature and second_best_feature and go through assert\n",
    "\n",
    "Hint: in **for** loop use **two** features to train the classifier. One of the two features is the found best feature from Task 1, the second feature is a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(first_best_feature == 'meanfun' and second_best_feature == 'Q75')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (4 points)\n",
    "\n",
    "Great! As you can guess, now your task is to find the best 3 features, the best 4 features, ...., the best 20 features :) Use the silimilar approach as in Task 2.\n",
    "\n",
    "For the each N best features calculate the classification accuracy. Plot dependency of the accuracy from N. Print list of the following pairs of values: (Name of the N-th best feature, accuracy).\n",
    "\n",
    "The output example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_features = np.arange(1, 21)\n",
    "n_best_names = feature_names # You need to find the correct order\n",
    "n_best_accuracies = np.random.rand(20) # You need to calculate these accuracies\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(n_best_features, n_best_accuracies, color='b', linewidth=3)\n",
    "plt.xticks(n_best_features, size=14)\n",
    "plt.xlabel(\"N best features\", size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylabel(\"Accuracy\", size=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.DataFrame()\n",
    "report['Name'] = feature_names[n_best_names]\n",
    "report['Accuracy'] = n_best_accuracies\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(report['Name'] == ['meanfun', 'Q75', 'IQR', 'sp.ent', 'minfun', 'mode', 'kurt', 'skew', 'maxfun', 'mindom', 'Q25', 'meanfreq', 'sd', 'sfm', 'median', 'centroid', 'maxdom', 'meandom', 'dfrange', 'modindx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (2 points)\n",
    "\n",
    "Well, now let's use PCA. Plot the dependency of the classification accuracy from the number of components of PCA. Vary the number of components from 1 to 20. Do you have the same accuracies for one PCA component and for the first best feature from Task 1? How do you explain it?\n",
    "\n",
    "The output example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = np.arange(1, 21)\n",
    "pca_accuracies = np.random.rand(20) # You need to calculate these accuracies\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(pca_components, pca_accuracies, color='b', linewidth=3)\n",
    "plt.xticks(pca_components, size=14)\n",
    "plt.xlabel(\"N components of PCA\", size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylabel(\"Accuracy\", size=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 (1 point)\n",
    "\n",
    "Plot explained variance and cumulative explained variance for the PCA at every number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
